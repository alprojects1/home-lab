## Quorum/Ceph Wiki:

**Setting up Ceph distributed storage typically requires a minimum of three nodes to achieve redundancy and fault tolerance. However, for a production-ready Ceph cluster, having more nodes is generally recommended to ensure better performance and higher availability.** With only three nodes, you can still set up a Ceph cluster, but it may not provide the optimal performance and fault tolerance that a larger cluster would offer. In a three-node setup, the failure of one node can still lead to a situation where the cluster is at risk if another node fails before the first one is restored.  

***Corosync Quorum Device*** (`QDevice`) is a daemon which runs on each cluster node. It provides a configured number of votes to the cluster’s quorum subsystem, based on an externally running third-party arbitrator’s decision. Its primary use is to allow a cluster to sustain more node failures than standard quorum rules allow.** This can be done safely as the external device can see all nodes and thus choose only one set of nodes to give its vote. This will only be done if said set of nodes can have quorum (`again`) after receiving the third-party vote. Currently, **only QDevice Net is supported as a third-party arbitrator. This is a daemon which provides a vote to a cluster partition, if it can reach the partition members over the network.** It will only give votes to one partition of a cluster at any time. It’s designed to support multiple clusters and is almost configuration and state free. New clusters are handled dynamically and no configuration file is needed on the host running a QDevice. 

***Replication and Redundancy*** With three nodes, you can set the replication factor to 3 (each piece of data is stored on all three nodes), ensuring that you can lose one node and still have all your data available (`QNAP also does this`). However, this setup limits your usable storage capacity to the size of a single node since all data is replicated across all nodes. Quorum: Ceph relies on a quorum of monitors (`MONs`) to make cluster decisions. With three monitors (`one on each node`), the loss of one node still allows the remaining two to achieve quorum and maintain cluster operations. 
  
   - ***Failure Scenarios*** - If one node fails, the remaining two nodes can still serve data, but the cluster will be in a degraded state. If another node fails before the first one is repaired and brought back online, you could potentially lose data. Performance may be impacted in a three-node setup due to the high level of replication and the limited number of nodes to handle the workload.
     
## HA Storage Solution: (Two Nodes) 

 - To achieve a high-availability (`HA`) storage solution with a two-node setup, we propose creating a custom storage architecture leveraging JBOD configurations with `SSDs` and `NFS/ISCSI` connections. This setup aims to deliver solid performance and reliability while maintaining high availability. 

 - ***Configuration*** Each node will be configured with a `JBOD` (`Just a Bunch of Disks`) setup utilizing SSDs to form a high-performance storage pool. each node has a isolated path to the `SAN` via `NFS` 4.1 ,and the nodes have 1G connection between eachother. This configuration allows each node to access the other’s storage over NFS, ensuring data accessibility and redundancy. A separate network interface card (`NIC`) and IP subnet is dedicated to the traffic, as mentioned above. This dedicated link will isolate storage traffic from regular network operations, thereby optimizing performance and reliability.
   
 - ***Implementation*** To achieve data synchronization between the nodes, we will implement block-level replication. `DRBD` (`Distributed Replicated Block Device`) will be used to mirror data in real-time, ensuring data consistency and availability. HA will be managed using Corosync and QDEVICE (`daemon that acts as 3rd vote`) These tools will oversee failover mechanisms and resource management, ensuring seamless operation even in the event of a node failure. 

##
> You must meet the following requirements before you start with `HA`: 1. at least three cluster nodes (`to get reliable quorum`) 2. shared storage for `VMs` and containers 3. hardware redundancy (`everywhere`) 4. use reliable “server” components 5. hardware watchdog - if not available we fall back to the Linux kernel software watchdog (`softdog`) 6. optional hardware fencing devices. One last thing, the external host needs network access to the cluster and to have a corosync-qnetd package available.  
